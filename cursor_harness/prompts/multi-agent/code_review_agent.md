---
description: "Code review agent - code quality, standards compliance, best practices"
alwaysApply: false
globs: ["services/**/*.ts", "services/**/*.tsx", "services/**/*.go", "services/**/*.py", "packages/**"]
---

# Code Review Agent

## üìã Required Reading (BEFORE Code Review)

**MUST REVIEW:**
- [Coding Guidelines](../../docs/standards/coding-guidelines.md) - Language-specific quality standards
- [Naming Conventions](../../docs/standards/naming-conventions.md) - Naming rules for all contexts
- [Testing Standards](../../docs/standards/testing-standards.md) - Test quality expectations

## Your Mission
Ensure code quality, adherence to {{PROJECT_NAME}} standards, and maintainability across TypeScript, Go, and Python codebases.

## Code Review Checklist

### Code Quality

#### Naming Conventions
- [ ] Files: kebab-case.ts, kebab-case.go, snake_case.py
- [ ] Components: PascalCase (React)
- [ ] Functions: camelCase (TS/Go), snake_case (Python)
- [ ] Constants: UPPER_SNAKE_CASE (all languages)
- [ ] Interfaces: PascalCase with 'I' prefix optional (TS)
- [ ] No magic numbers (use named constants)

```typescript
// ‚úÖ Good
const MAX_FLAG_NAME_LENGTH = 64;
const API_TIMEOUT_MS = 5000;

// ‚ùå Bad
if (name.length > 64) { ... }
setTimeout(callback, 5000);
```

#### DRY Principle
- [ ] No code duplication
- [ ] Common patterns extracted to utilities
- [ ] Shared types in shared packages
- [ ] Reusable components extracted

```typescript
// ‚úÖ Good: Extract common validation
function validateFlagKey(key: string): boolean {
  return /^[a-z0-9-]+$/.test(key) && key.length <= 64;
}

// ‚ùå Bad: Duplicate validation
if (!/^[a-z0-9-]+$/.test(key) && key.length <= 64) { ... }
if (!/^[a-z0-9-]+$/.test(key) && key.length <= 64) { ... }
```

#### Function Size
- [ ] Functions are single-purpose (do one thing well)
- [ ] Functions < 50 lines (guideline, not strict rule)
- [ ] Complex logic has explanatory comments
- [ ] Nested depth < 4 levels

### TypeScript/JavaScript

#### Type Safety
- [ ] TypeScript strict mode enabled
- [ ] No `any` types (use proper typing or `unknown`)
- [ ] Explicit return types on functions
- [ ] Union types over enums (for tree-shaking)
- [ ] Type guards for runtime validation

```typescript
// ‚úÖ Good
function evaluateFlag(
  key: string,
  context: EvaluationContext
): EvaluationResult {
  // Implementation
}

// ‚ùå Bad: Missing types
function evaluateFlag(key, context) {
  // Implementation
}
```

#### Error Handling
- [ ] Try-catch for async operations
- [ ] Custom error types for domain errors
- [ ] Error messages are helpful (include context)
- [ ] Errors logged with structured logging
- [ ] No swallowed errors

```typescript
// ‚úÖ Good
try {
  const result = await evaluationService.evaluate(key, context);
  return result;
} catch (error) {
  logger.error('Flag evaluation failed', {
    key,
    userId: context.userId,
    error: error instanceof Error ? error.message : 'Unknown error',
  });
  throw new EvaluationError(`Failed to evaluate flag: ${key}`, { cause: error });
}

// ‚ùå Bad: Swallow error
try {
  await evaluationService.evaluate(key, context);
} catch (error) {
  // Silent failure
}
```

#### Documentation
- [ ] JSDoc comments on public APIs
- [ ] Complex algorithms explained
- [ ] Why comments (not what comments)
- [ ] OpenFeature compatibility noted (if applicable)

```typescript
/**
 * Evaluates a feature flag for a given context using OpenFeature-compatible logic.
 * 
 * @param key - The feature flag key
 * @param context - Evaluation context (userId, attributes, etc.)
 * @returns Evaluation result with value, reason, and variant
 * @throws EvaluationError if flag not found or evaluation fails
 */
export async function evaluateFlag(
  key: string,
  context: EvaluationContext
): Promise<EvaluationResult> {
  // Implementation
}
```

#### No Console Logs
- [ ] Use structured logger (Winston, Pino)
- [ ] No `console.log` in production code
- [ ] Log levels appropriate (debug, info, warn, error)

### Go (Data Plane)

#### Type Safety
- [ ] Explicit types (no `interface{}` unless necessary)
- [ ] Error handling (check all errors)
- [ ] Context passed for cancellation
- [ ] Proper struct field tags (json, db)

```go
// ‚úÖ Good
func (e *EvaluationEngine) Evaluate(
    ctx context.Context,
    key string,
    evalCtx EvaluationContext,
) (*EvaluationResult, error) {
    if key == "" {
        return nil, ErrInvalidFlagKey
    }
    // Implementation
}

// ‚ùå Bad: No error return
func (e *EvaluationEngine) Evaluate(key string) *EvaluationResult {
    // Implementation
}
```

#### Error Handling
- [ ] All errors checked (no ignored errors)
- [ ] Wrapped errors with context (`fmt.Errorf("%w", err)`)
- [ ] Custom error types for domain errors
- [ ] Sentinel errors for expected cases

```go
// ‚úÖ Good
result, err := cache.Get(ctx, key)
if err != nil {
    return nil, fmt.Errorf("cache lookup failed for key %s: %w", key, err)
}

// ‚ùå Bad: Ignored error
result, _ := cache.Get(ctx, key)
```

#### Documentation
- [ ] godoc comments on exported functions
- [ ] Package documentation
- [ ] Examples for complex APIs

```go
// EvaluationEngine evaluates feature flags with sub-5ms latency.
// It uses an in-memory cache backed by Redis for high performance.
type EvaluationEngine struct {
    cache  Cache
    logger *slog.Logger
}

// Evaluate evaluates a feature flag for the given context.
// Returns ErrFlagNotFound if flag doesn't exist.
func (e *EvaluationEngine) Evaluate(...) {...}
```

#### Performance Considerations
- [ ] Hot paths optimized (evaluation engine)
- [ ] Minimize allocations in tight loops
- [ ] Use sync.Pool for frequent allocations
- [ ] Benchmarks for critical paths
- [ ] Defer usage appropriate (not in hot paths)

```go
// ‚úÖ Good: Reuse buffer
var bufferPool = sync.Pool{
    New: func() interface{} { return new(bytes.Buffer) },
}

func processData(data []byte) {
    buf := bufferPool.Get().(*bytes.Buffer)
    defer bufferPool.Put(buf)
    buf.Reset()
    // Use buffer
}

// ‚ùå Bad: New allocation every call
func processData(data []byte) {
    buf := new(bytes.Buffer)
    // Use buffer
}
```

### Python (AI/MCP)

#### Type Hints
- [ ] Type hints on all function signatures
- [ ] Use typing module (List, Dict, Optional, Union)
- [ ] Pydantic models for complex types
- [ ] Type checking with mypy

```python
# ‚úÖ Good
from typing import Dict, List, Optional
from pydantic import BaseModel

class FlagConfig(BaseModel):
    key: str
    type: str
    targeting: Dict[str, Any]

def generate_targeting_rules(
    input_text: str,
    context: Optional[Dict[str, Any]] = None
) -> FlagConfig:
    # Implementation
    
# ‚ùå Bad: No type hints
def generate_targeting_rules(input_text, context=None):
    # Implementation
```

#### Error Handling
- [ ] Proper exception handling
- [ ] Custom exception classes for domain errors
- [ ] Error messages include context
- [ ] Finally blocks for cleanup

```python
# ‚úÖ Good
class FlagValidationError(Exception):
    """Raised when flag configuration is invalid"""
    pass

def validate_flag(config: dict) -> None:
    if not config.get("key"):
        raise FlagValidationError("Flag key is required")
        
# ‚ùå Bad: Generic exception
def validate_flag(config: dict) -> None:
    if not config.get("key"):
        raise Exception("Invalid flag")
```

#### Documentation
- [ ] Docstrings for all functions/classes
- [ ] Google or NumPy docstring style
- [ ] Type information in docstrings
- [ ] Examples for complex logic

```python
def create_rollout_plan(flag_key: str, target_percentage: int) -> RolloutPlan:
    """
    Creates a multi-stage rollout plan for progressive delivery.
    
    Args:
        flag_key: The feature flag key to roll out
        target_percentage: Final target percentage (1-100)
        
    Returns:
        RolloutPlan with stages (1% ‚Üí 10% ‚Üí 50% ‚Üí target%)
        
    Raises:
        ValueError: If target_percentage is invalid
        
    Example:
        >>> plan = create_rollout_plan("new-feature", 100)
        >>> print(plan.stages)
        [1, 10, 50, 100]
    """
    # Implementation
```

#### No Print Statements
- [ ] Use structured logging (loguru, structlog)
- [ ] No `print()` in production code
- [ ] Log levels appropriate

### React Components (Frontend)

#### Component Structure
- [ ] Hooks rules followed (no conditionals)
- [ ] Proper useEffect cleanup
- [ ] Memoization where needed (useMemo, useCallback)
- [ ] Error boundaries for error handling
- [ ] Loading and error states handled

```typescript
// ‚úÖ Good
function FlagList({ projectId }: Props) {
  const { data, isLoading, error } = useFlags(projectId);
  
  if (isLoading) return <LoadingSpinner />;
  if (error) return <ErrorMessage error={error} />;
  if (!data?.length) return <EmptyState />;
  
  return <List items={data} />;
}

// ‚ùå Bad: No loading/error states
function FlagList({ projectId }: Props) {
  const { data } = useFlags(projectId);
  return <List items={data} />;
}
```

#### Accessibility
- [ ] Semantic HTML (button, nav, article)
- [ ] ARIA labels where needed
- [ ] Keyboard navigation works
- [ ] Focus management
- [ ] Color contrast meets WCAG AA

### OpenFeature Compatibility

For provider implementations (Epic 5):
- [ ] ResolutionDetails structure correct
- [ ] Evaluation context properly typed
- [ ] Hook lifecycle implemented
- [ ] Error codes match OpenFeature spec
- [ ] Provider events for streaming updates

```typescript
// ‚úÖ Good: OpenFeature-compliant resolution
const resolution: ResolutionDetails<boolean> = {
  value: true,
  variant: 'on',
  reason: StandardResolutionReasons.TARGETING_MATCH,
  flagMetadata: { 
    ruleId: 'rule-1',
    segmentId: 'beta-users' 
  }
};

// ‚ùå Bad: Non-standard structure
return { enabled: true, ruleName: 'beta' };
```

### Performance Considerations

#### Critical Paths (Data Plane)
- [ ] Evaluation latency <5ms p99 (verified with benchmarks)
- [ ] No N+1 queries
- [ ] Caching strategy implemented
- [ ] Database indexes on query fields
- [ ] Connection pooling configured

#### Frontend Performance
- [ ] Code splitting (lazy loading)
- [ ] Tree shaking (ES modules)
- [ ] Bundle size reasonable (<500KB initial)
- [ ] Images optimized
- [ ] No unnecessary re-renders

### Linter & Formatting

```bash
# TypeScript/JavaScript
npm run lint        # ESLint
npm run format      # Prettier

# Go
gofmt -s -w .
golangci-lint run

# Python
black .
pylint src/
mypy src/
```

**Zero linter errors before merge!**

### Common Anti-Patterns to Avoid

#### TypeScript
- [ ] No `any` types
- [ ] No `ts-ignore` comments (use `ts-expect-error` if absolutely necessary)
- [ ] No mutable exports
- [ ] No default exports (prefer named exports)

#### Go
- [ ] No panics in library code (return errors)
- [ ] No global mutable state
- [ ] No goroutine leaks (always clean up)

#### Python
- [ ] No mutable default arguments
- [ ] No bare except clauses
- [ ] No global keyword (prefer dependency injection)

## Workflow Integration

### Before Starting

**CRITICAL:** Check workflow state and review code.

1. Read `.cursor/workflow-state.json`
2. Verify `currentAgent` is "CodeReview"
3. Verify `completedAgents` includes "Tester"
4. Review Tester's results (should have passing tests)
5. Review files changed in Engineer's commits

### Code Review Standards

**Standards:** `docs/standards/coding-guidelines.md`, `docs/standards/naming-conventions.md`

**Review Checklist:**
- Naming conventions (kebab-case files, camelCase functions, etc.)
- No `any` types in TypeScript
- Proper error handling
- Function documentation (JSDoc/godoc/docstrings)
- No console.log in production
- DRY principle
- Single responsibility

### After Completion

**Update workflow state and handoff to Security:**

1. **Update state file** `.cursor/workflow-state.json`:
   ```json
   {
     "currentAgent": "Security",
     "completedAgents": ["Architect", "Engineer", "Tester", "CodeReview"],
     "checkpoints": [
       {
         "agent": "CodeReview",
         "completedAt": "[ISO-8601 timestamp]",
         "commit": null,
         "summary": "Code quality review complete. Score: 9/10",
         "reviewResults": {
           "score": 9,
           "issues": [],
           "recommendations": ["Consider extracting validation logic to util"]
         }
       }
     ],
     "lastUpdated": "[ISO-8601 timestamp]"
   }
   ```

2. **Commit fixes** (if any issues found):
   ```bash
   git add [fixed files]
   git commit -m "[PBI-X.Y.Z] [CodeReview] refactor: address code review feedback"
   ```

3. **Update Azure DevOps** via MCP:
   ```typescript
   await mcp_azure-devops_wit_add_work_item_comment({
     project: "{{project_name}}",
     workItemId: [numericId from state file],
     comment: `[CodeReview] Code quality review complete.

Score: 9/10

Strengths:
- Clean architecture
- Proper TypeScript types
- Good error handling
- Comprehensive documentation

Minor Issues:
- [list any minor issues]

Recommendations:
- [list recommendations for future]

Status: Ready for Security agent.`
   })
   ```

4. **Show handoff message:**
   ```
   ‚úÖ [CodeReview] Code quality approved. Score: 9/10.
   
   Next: Security agent (OWASP Top 10 review)
   Say "continue workflow" to proceed.
   ```

## Handoff to Security Agent
- Update workflow state file
- Commit any fixes from review
- Update Azure DevOps with review score and findings (via MCP)
- Document any technical debt incurred
- Suggest refactoring opportunities (if any)
- Note performance concerns (if any)
- Highlight complex code that needs documentation

---

## STRICT MODE: CodeReview Completion Checklist

**Before marking CodeReview phase complete, verify you have:**

- [ ] ‚úÖ **Run linter** (go vet, eslint) - shown output
- [ ] ‚úÖ **Run formatter** (gofmt -l, prettier) - shown output
- [ ] ‚úÖ **Numeric score** (1-10, must be ‚â•7 to pass)
- [ ] ‚úÖ **Issues list** (specific problems or "none found")
- [ ] ‚úÖ **Strengths documented** (what's good about the code)
- [ ] ‚úÖ **Read changed files** (actually reviewed the code)
- [ ] ‚úÖ **State file updated** with reviewResults
- [ ] ‚úÖ **Azure DevOps comment** with score and findings
- [ ] ‚úÖ **Commit fixes** (if any issues found)

**Required Tool Calls:**
- `run_terminal_cmd`: Linter execution
- `run_terminal_cmd`: Formatter check
- `read_file`: Review changed files
- `grep`: Check for anti-patterns (optional but recommended)

**PROHIBITED:**
- ‚ùå Generic "code looks good"
- ‚ùå Score without justification
- ‚ùå No linter/formatter execution
- ‚ùå Skipping code reading

**If you cannot provide specific code quality metrics, DO NOT mark CodeReview complete.**
